{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-22 06:12:05,672] INFO - USING amp_min/amp_max: -1.500e+00 / 1.500e+00 | n_bins=256\n",
      "[2026-02-22 06:12:05,673] INFO - Architecture: k=2, layers=14, RF=1000 samples (1.000s)\n",
      "[2026-02-22 06:12:05,674] INFO - LOAD DATA...\n",
      "[2026-02-22 06:12:05,783] INFO - sfreq=1000.0 | n_epochs=271 | load_dt=0.11s\n",
      "[2026-02-22 06:12:05,784] INFO - splits (uLAR-style): train=217 val=27 test=27\n",
      "[2026-02-22 06:12:05,802] INFO - device=cuda\n",
      "[2026-02-22 06:12:05,924] INFO - DEBUG train_ds bins: unique=234 minbin=12 maxbin=245\n",
      "[2026-02-22 06:12:05,994] INFO - BASELINE val_unigram_ce=4.955381e+00 (lower is better)\n",
      "[2026-02-22 06:12:05,995] INFO - Checkpoints: c:\\Users\\ricca\\hackathon_IMAC\\model_output\\checkpoints\n",
      "[2026-02-22 06:12:05,996] INFO - TRAIN for up to 150 epochs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train global min/max: -0.9999999999999084 0.9999999999999479\n",
      "train per-series min (p1/p50/p99): [-1.         -0.18506235 -0.03250742]\n",
      "train per-series max (p1/p50/p99): [0.74171433 1.         1.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-22 06:12:07,055] INFO - Resuming from checkpoint: c:\\Users\\ricca\\hackathon_IMAC\\model_output\\checkpoints\\last.pt\n",
      "[2026-02-22 06:12:07,073] INFO - Resume @ epoch=149 | best_val=3.607766e+00 (epoch=143) | bad=5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 361\u001b[39m\n\u001b[32m    359\u001b[39m \u001b[32m0\u001b[39m\n\u001b[32m    360\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 317\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    314\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCheckpoints: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mckpt_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    315\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTRAIN for up to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg.epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m epochs...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m model = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreceptive_field\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwb_run\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m    \u001b[49m\u001b[43mckpt_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    329\u001b[39m torch.save({\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model.state_dict(), \u001b[33m\"\u001b[39m\u001b[33mcfg\u001b[39m\u001b[33m\"\u001b[39m: asdict(cfg)}, out_dir / \u001b[33m\"\u001b[39m\u001b[33mfinal.pt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    330\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSaved: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_dir/\u001b[33m'\u001b[39m\u001b[33mfinal.pt\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricca\\hackathon_IMAC\\core\\training.py:82\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_ds, val_loader, rf, cfg, device, logger, wb_run, ckpt_dir)\u001b[39m\n\u001b[32m     80\u001b[39m loss.backward()\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cfg.grad_clip \u001b[38;5;129;01mand\u001b[39;00m cfg.grad_clip > \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad_clip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m opt.step()\n\u001b[32m     84\u001b[39m tr_losses.append(\u001b[38;5;28mfloat\u001b[39m(loss.item()))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\utils\\clip_grad.py:42\u001b[39m, in \u001b[36m_no_grad.<locals>._no_grad_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_no_grad_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     41\u001b[39m         \u001b[38;5;66;03m# pyrefly: ignore [invalid-param-spec]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\utils\\clip_grad.py:231\u001b[39m, in \u001b[36mclip_grad_norm_\u001b[39m\u001b[34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[39m\n\u001b[32m    226\u001b[39m         warnings.warn(\n\u001b[32m    227\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`parameters` is an empty generator, no gradient clipping will occur.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    228\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,\n\u001b[32m    229\u001b[39m         )\n\u001b[32m    230\u001b[39m grads = [p.grad \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters \u001b[38;5;28;01mif\u001b[39;00m p.grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m total_norm = \u001b[43m_get_total_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_if_nonfinite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m _clip_grads_with_norm_(parameters, max_norm, total_norm, foreach)\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m total_norm\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\utils\\clip_grad.py:42\u001b[39m, in \u001b[36m_no_grad.<locals>._no_grad_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_no_grad_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     41\u001b[39m         \u001b[38;5;66;03m# pyrefly: ignore [invalid-param-spec]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\utils\\clip_grad.py:96\u001b[39m, in \u001b[36m_get_total_norm\u001b[39m\u001b[34m(tensors, norm_type, error_if_nonfinite, foreach)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (device, _), ([device_tensors], _) \u001b[38;5;129;01min\u001b[39;00m grouped_tensors.items():\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (foreach \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m _has_foreach_support(device_tensors, device)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m     94\u001b[39m         foreach \u001b[38;5;129;01mand\u001b[39;00m _device_has_foreach_support(device)\n\u001b[32m     95\u001b[39m     ):\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m         norms.extend(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_foreach_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     97\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m foreach:\n\u001b[32m     98\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     99\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mforeach=True was passed, but can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt use the foreach API on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice.type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m tensors\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    100\u001b[39m         )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from dataclasses import asdict\n",
    "import time\n",
    "import time\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from core.wavenet import WaveNetCategorical\n",
    "from util.metrics import save_random_postcue_plots\n",
    "from util.util import *\n",
    "from util.datasets import RandomWaveNetSegments\n",
    "from util.Cfg import Cfg\n",
    "from util.wavenet import dilations_1s_context, receptive_field\n",
    "from core.training import train_model, eval_streaming_latency_comp, eval_postcue_window\n",
    "\n",
    "import yaml\n",
    "\n",
    "config = yaml.safe_load(open(\"config.yaml\"))\n",
    "\n",
    "def main():\n",
    "\n",
    "    args = Cfg()\n",
    "\n",
    "    out_dir = Path.cwd() / config[\"training\"][\"out_dir\"]\n",
    "    logger = setup_logger(out_dir)\n",
    "    set_seed(0)\n",
    "\n",
    "    cfg = Cfg()\n",
    "\n",
    "    logger.info(f\"USING amp_min/amp_max: {cfg.amp_min:.3e} / {cfg.amp_max:.3e} | n_bins={cfg.n_bins}\")\n",
    "\n",
    "    dils = dilations_1s_context()\n",
    "    cfg.receptive_field = int(receptive_field(cfg.kernel_size, dils))\n",
    "    logger.info(\n",
    "        f\"Architecture: k={cfg.kernel_size}, \"\n",
    "        f\"layers={len(dils)}, \"\n",
    "        f\"RF={cfg.receptive_field} samples \"\n",
    "        f\"({cfg.receptive_field/cfg.sfreq:.3f}s)\"\n",
    "    )\n",
    "\n",
    "    logger.info(\"LOAD DATA...\")\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    epochs_1d = load_epochs_from_npz(config[\"data_preparation\"][\"npz_data_output\"])\n",
    "    N = len(epochs_1d)\n",
    "    logger.info(f\"sfreq={cfg.sfreq} | n_epochs={N} | load_dt={time.perf_counter()-t0:.2f}s\")\n",
    "\n",
    "    train_ids, val_ids, test_ids = split_epochs(\n",
    "        N, train_frac=args.train_frac, val_frac=args.val_frac, seed=args.split_seed\n",
    "    )\n",
    "    tr_list = [epochs_1d[int(i)] for i in train_ids]\n",
    "    va_list_full = [epochs_1d[int(i)] for i in val_ids]\n",
    "    te_list_full = [epochs_1d[int(i)] for i in test_ids]\n",
    "\n",
    "    logger.info(f\"splits (uLAR-style): train={len(tr_list)} val={len(va_list_full)} test={len(te_list_full)}\")\n",
    "\n",
    "    max_eval = int(args.eval_max_epochs) if args.eval_max_epochs is not None else None\n",
    "    va_list = va_list_full[:max_eval] if max_eval is not None else va_list_full\n",
    "    te_list = te_list_full[:max_eval] if max_eval is not None else te_list_full\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"device={device}\")\n",
    "\n",
    "    run_name = f\"consumption\"\n",
    "    wb = wandb_init(asdict(cfg), run_name=run_name)\n",
    "\n",
    "    model = WaveNetCategorical(n_bins=cfg.n_bins, n_filters=cfg.n_filters, kernel_size=cfg.kernel_size)\n",
    "\n",
    "    if args.ckpt is not None:\n",
    "        bundle = torch.load(args.ckpt, map_location=\"cpu\", weights_only=False)\n",
    "        if isinstance(bundle, dict) and \"model\" in bundle:\n",
    "            model.load_state_dict(bundle[\"model\"])\n",
    "        else:\n",
    "            model.load_state_dict(bundle)\n",
    "        logger.info(f\"Loaded checkpoint: {args.ckpt}\")\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            logger.info(f\"[GPU CHECK] cuda mem allocated MB={torch.cuda.memory_allocated()/1024**2:.1f}\")\n",
    "\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    if not args.eval_only:\n",
    "        ckpt_dir = out_dir / \"checkpoints\"\n",
    "\n",
    "        mins = [float(np.min(x)) for x in tr_list]\n",
    "        maxs = [float(np.max(x)) for x in tr_list]\n",
    "        print(\"train global min/max:\", min(mins), max(maxs))\n",
    "        print(\"train per-series min (p1/p50/p99):\", np.percentile(mins, [1,50,99]))\n",
    "        print(\"train per-series max (p1/p50/p99):\", np.percentile(maxs, [1,50,99]))\n",
    "\n",
    "        train_ds = RandomWaveNetSegments(\n",
    "            epochs_1d=tr_list,\n",
    "            seq_len=cfg.seq_len,\n",
    "            n_samples=cfg.train_samples_per_epoch,\n",
    "            amp_min=cfg.amp_min,\n",
    "            amp_max=cfg.amp_max,\n",
    "            n_bins=cfg.n_bins,\n",
    "            rng=np.random.default_rng(1),\n",
    "        )\n",
    "\n",
    "        ys = []\n",
    "        for i in range(10):\n",
    "            _, y0 = train_ds[i]\n",
    "            ys.append(y0)\n",
    "        ycat = torch.cat(ys)\n",
    "        logger.info(\n",
    "            f\"DEBUG train_ds bins: unique={int(torch.unique(ycat).numel())} \"\n",
    "            f\"minbin={int(ycat.min())} maxbin={int(ycat.max())}\"\n",
    "        )\n",
    "\n",
    "\n",
    "        val_ds_tmp = RandomWaveNetSegments(\n",
    "            epochs_1d=va_list_full,\n",
    "            seq_len=cfg.seq_len,\n",
    "            n_samples=cfg.val_samples_fixed,\n",
    "            amp_min=cfg.amp_min,\n",
    "            amp_max=cfg.amp_max,\n",
    "            n_bins=cfg.n_bins,\n",
    "            rng=np.random.default_rng(2),\n",
    "        )\n",
    "        fixed_pairs = list(val_ds_tmp.pairs)\n",
    "        val_ds = RandomWaveNetSegments(\n",
    "            epochs_1d=va_list_full,\n",
    "            seq_len=cfg.seq_len,\n",
    "            n_samples=len(fixed_pairs),\n",
    "            amp_min=cfg.amp_min,\n",
    "            amp_max=cfg.amp_max,\n",
    "            n_bins=cfg.n_bins,\n",
    "            rng=np.random.default_rng(2),\n",
    "            fixed_pairs=fixed_pairs,\n",
    "        )\n",
    "        val_loader = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False, num_workers=0, drop_last=False)\n",
    "\n",
    "\n",
    "        rf = cfg.receptive_field\n",
    "        counts = torch.zeros(cfg.n_bins, dtype=torch.long)\n",
    "        total = 0\n",
    "        for _, yb in val_loader:\n",
    "            y_v = yb[:, rf:].reshape(-1)  # ignora i primi rf come fai in training\n",
    "            counts += torch.bincount(y_v.cpu(), minlength=cfg.n_bins)\n",
    "            total += int(y_v.numel())\n",
    "        p = counts.float() / max(total, 1)\n",
    "        unigram_ce = (-(p.clamp_min(1e-12).log()) * counts.float()).sum() / max(total, 1)\n",
    "        logger.info(f\"BASELINE val_unigram_ce={unigram_ce.item():.6e} (lower is better)\")\n",
    "\n",
    "\n",
    "        logger.info(f\"Checkpoints: {ckpt_dir}\")\n",
    "        logger.info(f\"TRAIN for up to {cfg.epochs} epochs...\")\n",
    "\n",
    "        model = train_model(\n",
    "            model=model,\n",
    "            train_ds=train_ds,\n",
    "            val_loader=val_loader,\n",
    "            rf=cfg.receptive_field,\n",
    "            cfg=cfg,\n",
    "            device=device,\n",
    "            logger=logger,\n",
    "            wb_run=wb,\n",
    "            ckpt_dir=ckpt_dir,\n",
    "        )\n",
    "\n",
    "        torch.save({\"model\": model.state_dict(), \"cfg\": asdict(cfg)}, out_dir / \"final.pt\")\n",
    "        logger.info(f\"Saved: {out_dir/'final.pt'}\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    \n",
    "    import argparse\n",
    "from dataclasses import asdict\n",
    "import time\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from core.wavenet import WaveNetCategorical\n",
    "from util.metrics import save_random_postcue_plots\n",
    "from util.util import *\n",
    "from util.datasets import RandomWaveNetSegments\n",
    "from util.Cfg import Cfg\n",
    "from util.wavenet import dilations_1s_context, receptive_field\n",
    "from core.training import train_model, eval_streaming_latency_comp, eval_postcue_window\n",
    "\n",
    "import yaml\n",
    "\n",
    "config = yaml.safe_load(open(\"config.yaml\"))\n",
    "\n",
    "def main():\n",
    "\n",
    "    args = Cfg()\n",
    "\n",
    "    out_dir = Path.cwd() / config[\"training\"][\"out_dir\"]\n",
    "    logger = setup_logger(out_dir)\n",
    "    set_seed(0)\n",
    "\n",
    "    cfg = Cfg()\n",
    "\n",
    "    logger.info(f\"USING amp_min/amp_max: {cfg.amp_min:.3e} / {cfg.amp_max:.3e} | n_bins={cfg.n_bins}\")\n",
    "\n",
    "    dils = dilations_1s_context()\n",
    "    cfg.receptive_field = int(receptive_field(cfg.kernel_size, dils))\n",
    "    logger.info(\n",
    "        f\"Architecture: k={cfg.kernel_size}, \"\n",
    "        f\"layers={len(dils)}, \"\n",
    "        f\"RF={cfg.receptive_field} samples \"\n",
    "        f\"({cfg.receptive_field/cfg.sfreq:.3f}s)\"\n",
    "    )\n",
    "\n",
    "    logger.info(\"LOAD DATA...\")\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    epochs_1d = load_epochs_from_npz(config[\"data_preparation\"][\"npz_data_output\"])\n",
    "    N = len(epochs_1d)\n",
    "    logger.info(f\"sfreq={cfg.sfreq} | n_epochs={N} | load_dt={time.perf_counter()-t0:.2f}s\")\n",
    "\n",
    "    train_ids, val_ids, test_ids = split_epochs(\n",
    "        N, train_frac=args.train_frac, val_frac=args.val_frac, seed=args.split_seed\n",
    "    )\n",
    "    tr_list = [epochs_1d[int(i)] for i in train_ids]\n",
    "    va_list_full = [epochs_1d[int(i)] for i in val_ids]\n",
    "    te_list_full = [epochs_1d[int(i)] for i in test_ids]\n",
    "\n",
    "    logger.info(f\"splits (uLAR-style): train={len(tr_list)} val={len(va_list_full)} test={len(te_list_full)}\")\n",
    "\n",
    "    max_eval = int(args.eval_max_epochs) if args.eval_max_epochs is not None else None\n",
    "    va_list = va_list_full[:max_eval] if max_eval is not None else va_list_full\n",
    "    te_list = te_list_full[:max_eval] if max_eval is not None else te_list_full\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"device={device}\")\n",
    "\n",
    "    run_name = f\"consumption\"\n",
    "    wb = wandb_init(asdict(cfg), run_name=run_name)\n",
    "\n",
    "    model = WaveNetCategorical(n_bins=cfg.n_bins, n_filters=cfg.n_filters, kernel_size=cfg.kernel_size)\n",
    "\n",
    "    if args.ckpt is not None:\n",
    "        bundle = torch.load(args.ckpt, map_location=\"cpu\", weights_only=False)\n",
    "        if isinstance(bundle, dict) and \"model\" in bundle:\n",
    "            model.load_state_dict(bundle[\"model\"])\n",
    "        else:\n",
    "            model.load_state_dict(bundle)\n",
    "        logger.info(f\"Loaded checkpoint: {args.ckpt}\")\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            logger.info(f\"[GPU CHECK] cuda mem allocated MB={torch.cuda.memory_allocated()/1024**2:.1f}\")\n",
    "\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    if not args.eval_only:\n",
    "        ckpt_dir = out_dir / \"checkpoints\"\n",
    "\n",
    "        mins = [float(np.min(x)) for x in tr_list]\n",
    "        maxs = [float(np.max(x)) for x in tr_list]\n",
    "        print(\"train global min/max:\", min(mins), max(maxs))\n",
    "        print(\"train per-series min (p1/p50/p99):\", np.percentile(mins, [1,50,99]))\n",
    "        print(\"train per-series max (p1/p50/p99):\", np.percentile(maxs, [1,50,99]))\n",
    "\n",
    "        train_ds = RandomWaveNetSegments(\n",
    "            epochs_1d=tr_list,\n",
    "            seq_len=cfg.seq_len,\n",
    "            n_samples=cfg.train_samples_per_epoch,\n",
    "            amp_min=cfg.amp_min,\n",
    "            amp_max=cfg.amp_max,\n",
    "            n_bins=cfg.n_bins,\n",
    "            rng=np.random.default_rng(1),\n",
    "        )\n",
    "\n",
    "        ys = []\n",
    "        for i in range(10):\n",
    "            _, y0 = train_ds[i]\n",
    "            ys.append(y0)\n",
    "        ycat = torch.cat(ys)\n",
    "        logger.info(\n",
    "            f\"DEBUG train_ds bins: unique={int(torch.unique(ycat).numel())} \"\n",
    "            f\"minbin={int(ycat.min())} maxbin={int(ycat.max())}\"\n",
    "        )\n",
    "\n",
    "\n",
    "        val_ds_tmp = RandomWaveNetSegments(\n",
    "            epochs_1d=va_list_full,\n",
    "            seq_len=cfg.seq_len,\n",
    "            n_samples=cfg.val_samples_fixed,\n",
    "            amp_min=cfg.amp_min,\n",
    "            amp_max=cfg.amp_max,\n",
    "            n_bins=cfg.n_bins,\n",
    "            rng=np.random.default_rng(2),\n",
    "        )\n",
    "        fixed_pairs = list(val_ds_tmp.pairs)\n",
    "        val_ds = RandomWaveNetSegments(\n",
    "            epochs_1d=va_list_full,\n",
    "            seq_len=cfg.seq_len,\n",
    "            n_samples=len(fixed_pairs),\n",
    "            amp_min=cfg.amp_min,\n",
    "            amp_max=cfg.amp_max,\n",
    "            n_bins=cfg.n_bins,\n",
    "            rng=np.random.default_rng(2),\n",
    "            fixed_pairs=fixed_pairs,\n",
    "        )\n",
    "        val_loader = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False, num_workers=0, drop_last=False)\n",
    "\n",
    "\n",
    "        rf = cfg.receptive_field\n",
    "        counts = torch.zeros(cfg.n_bins, dtype=torch.long)\n",
    "        total = 0\n",
    "        for _, yb in val_loader:\n",
    "            y_v = yb[:, rf:].reshape(-1)  # ignora i primi rf come fai in training\n",
    "            counts += torch.bincount(y_v.cpu(), minlength=cfg.n_bins)\n",
    "            total += int(y_v.numel())\n",
    "        p = counts.float() / max(total, 1)\n",
    "        unigram_ce = (-(p.clamp_min(1e-12).log()) * counts.float()).sum() / max(total, 1)\n",
    "        logger.info(f\"BASELINE val_unigram_ce={unigram_ce.item():.6e} (lower is better)\")\n",
    "\n",
    "\n",
    "        logger.info(f\"Checkpoints: {ckpt_dir}\")\n",
    "        logger.info(f\"TRAIN for up to {cfg.epochs} epochs...\")\n",
    "\n",
    "        model = train_model(\n",
    "            model=model,\n",
    "            train_ds=train_ds,\n",
    "            val_loader=val_loader,\n",
    "            rf=cfg.receptive_field,\n",
    "            cfg=cfg,\n",
    "            device=device,\n",
    "            logger=logger,\n",
    "            wb_run=wb,\n",
    "            ckpt_dir=ckpt_dir,\n",
    "        )\n",
    "\n",
    "        torch.save({\"model\": model.state_dict(), \"cfg\": asdict(cfg)}, out_dir / \"final.pt\")\n",
    "        logger.info(f\"Saved: {out_dir/'final.pt'}\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    \n",
    "    m_val = eval_postcue_window(model, va_list, cfg, device, decode=args.decode)\n",
    "    m_test = eval_postcue_window(model, te_list, cfg, device, decode=args.decode)\n",
    "\n",
    "    save_random_postcue_plots(\n",
    "        model=model,\n",
    "        epochs_1d=te_list,\n",
    "        cfg=cfg,\n",
    "        device=device,\n",
    "        out_dir=out_dir,\n",
    "        n_plots=10,\n",
    "        split_name=\"test\",\n",
    "        seed=0,     \n",
    "    )\n",
    "    logger.info(f\"Saved 10 random post-cue plots to: {out_dir}\")\n",
    "\n",
    "    if wb is not None:\n",
    "        wb.finish()\n",
    "\n",
    "    logger.info(\"DONE\")\n",
    "    if wb is not None:\n",
    "        wb.finish()\n",
    "\n",
    "    logger.info(\"DONE\")\n",
    "\n",
    "0\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-22 04:00:20,188] INFO - USING amp_min/amp_max: 0.000e+00 / 1.500e+01 | n_bins=256\n",
      "[2026-02-22 04:00:20,189] INFO - Architecture: k=2, layers=14, RF=1000 samples (1.000s)\n",
      "[2026-02-22 04:00:20,189] INFO - LOAD DATA...\n",
      "[2026-02-22 04:00:20,250] INFO - sfreq=1000.0 | n_epochs=270 | load_dt=0.06s\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Cfg' object has no attribute 'train_frac'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 233\u001B[39m\n\u001B[32m    229\u001B[39m     logger.info(\u001B[33m\"\u001B[39m\u001B[33mDONE\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    232\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[34m__name__\u001B[39m == \u001B[33m\"\u001B[39m\u001B[33m__main__\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m233\u001B[39m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 48\u001B[39m, in \u001B[36mmain\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m     44\u001B[39m N = \u001B[38;5;28mlen\u001B[39m(epochs_1d)\n\u001B[32m     45\u001B[39m logger.info(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33msfreq=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcfg.sfreq\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m | n_epochs=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mN\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m | load_dt=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtime.perf_counter()-t0\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33ms\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     47\u001B[39m train_ids, val_ids, test_ids = split_epochs(\n\u001B[32m---> \u001B[39m\u001B[32m48\u001B[39m     N, train_frac=\u001B[43margs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain_frac\u001B[49m, val_frac=args.val_frac, seed=args.split_seed\n\u001B[32m     49\u001B[39m )\n\u001B[32m     50\u001B[39m tr_list = [epochs_1d[\u001B[38;5;28mint\u001B[39m(i)] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m train_ids]\n\u001B[32m     51\u001B[39m va_list_full = [epochs_1d[\u001B[38;5;28mint\u001B[39m(i)] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m val_ids]\n",
      "\u001B[31mAttributeError\u001B[39m: 'Cfg' object has no attribute 'train_frac'"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

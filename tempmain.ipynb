{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-22T00:15:56.173705Z",
     "start_time": "2026-02-22T00:07:51.724438Z"
    }
   },
   "source": [
    "\n",
    "from core.training import train_model\n",
    "from util.datasets import RandomWaveNetSegments\n",
    "from util.wavenet import dilations_1s_context, receptive_field\n",
    "from util.metrics import metrics_1d\n",
    "from util.quantization import mu_law_decode_np\n",
    "from core.wavenet import WaveNetCategorical\n",
    "\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "\n",
    "config = yaml.safe_load(open(\"config.yaml\", \"r\"))\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "WINDOW_ROOT = Path.cwd() / config[\"training\"][\"training_windows_dataset\"]\n",
    "WINDOW_MONTHS_LIST = (3, )\n",
    "VALUE_COL = \"max\"\n",
    "\n",
    "RUNS_DIR = Path(\"trained_wavenet_kfold_tests\")\n",
    "RECURSIVE = True\n",
    "\n",
    "# K-fold settings\n",
    "K_FOLDS = 3                     # change to 3 if runs take too long\n",
    "TRAIN_FRAC_WITHIN_FILE = None\n",
    "MIN_LEN_PADDING = 0             # extra length beyond seq_len + 10 if you want stricter filtering\n",
    "\n",
    "# Training settings per fold\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    n_bins: int = 256\n",
    "    kernel_size: int = 2\n",
    "    n_filters: int = 16\n",
    "\n",
    "    lr: float = 1e-3\n",
    "    lr_decay_gamma: float = 0.99\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 200\n",
    "    train_samples_per_epoch: int = 6000\n",
    "    val_samples_fixed: int = 1500\n",
    "    early_stop_patience: int = 10\n",
    "    grad_clip: float = 1.0\n",
    "\n",
    "    amp_min: float = -3.0\n",
    "    amp_max: float =  3.0\n",
    "\n",
    "    receptive_field: int = 1024\n",
    "    seq_len: int = 0\n",
    "\n",
    "\n",
    "def setup_logger(out_dir: Path, name: str) -> logging.Logger:\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    lg = logging.getLogger(name)\n",
    "    lg.setLevel(logging.INFO)\n",
    "    lg.handlers.clear()\n",
    "    fmt = logging.Formatter(\"[%(asctime)s] %(levelname)s - %(message)s\")\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setFormatter(fmt)\n",
    "    lg.addHandler(ch)\n",
    "    fh = logging.FileHandler(out_dir / \"run.log\")\n",
    "    fh.setFormatter(fmt)\n",
    "    lg.addHandler(fh)\n",
    "    return lg\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Window loading helpers\n",
    "# ----------------------------\n",
    "def iter_window_files(ds_dir: Path) -> list[Path]:\n",
    "    pats = [\"**/*.parquet\", \"**/*.csv.gz\", \"**/*.csv\"] if RECURSIVE else [\"*.parquet\", \"*.csv.gz\", \"*.csv\"]\n",
    "    files = []\n",
    "    for pat in pats:\n",
    "        files.extend(ds_dir.glob(pat))\n",
    "    files = sorted(set(files))\n",
    "\n",
    "    # ✅ keep only LOAD files by filename (adjust keywords if your naming differs)\n",
    "    def is_load_file(p: Path) -> bool:\n",
    "        name = p.name.lower()\n",
    "        return (\"load\" in name) and (\"pv\" not in name)\n",
    "\n",
    "    files = [p for p in files if is_load_file(p)]\n",
    "    return files\n",
    "\n",
    "\n",
    "def read_window_file(path: Path) -> pd.DataFrame:\n",
    "    if path.suffix.lower() == \".parquet\":\n",
    "        return pd.read_parquet(path)\n",
    "    if path.name.lower().endswith(\".csv.gz\"):\n",
    "        return pd.read_csv(path, compression=\"gzip\")\n",
    "    if path.suffix.lower() == \".csv\":\n",
    "        return pd.read_csv(path)\n",
    "    raise ValueError(f\"Unsupported window file: {path}\")\n",
    "\n",
    "\n",
    "def load_series_from_window_file(\n",
    "    path: Path,\n",
    "    value_col: str = \"max\",\n",
    "    require_metric: str | None = \"load_power\",  # ✅ filter PV via metric column if present\n",
    ") -> np.ndarray | None:\n",
    "    df = read_window_file(path)\n",
    "\n",
    "    # ✅ if there is a metric column, only keep load_power rows\n",
    "    if require_metric is not None and \"metric\" in df.columns:\n",
    "        df = df[df[\"metric\"].astype(str).str.lower() == require_metric.lower()]\n",
    "        if df.empty:\n",
    "            return None\n",
    "\n",
    "    if \"utc\" in df.columns:\n",
    "        df[\"utc\"] = pd.to_datetime(df[\"utc\"], errors=\"coerce\")\n",
    "        df = df.dropna(subset=[\"utc\"]).sort_values(\"utc\")\n",
    "\n",
    "    if value_col not in df.columns:\n",
    "        return None\n",
    "\n",
    "    y = pd.to_numeric(df[value_col], errors=\"coerce\").dropna().to_numpy(dtype=np.float64)\n",
    "    if y.size < 10:\n",
    "        return None\n",
    "\n",
    "    # ✅ vertical normalization ONLY (no scaling)\n",
    "    y = y - float(np.mean(y))\n",
    "    return y\n",
    "\n",
    "\n",
    "def files_to_series_list(files: list[Path], min_len: int, value_col: str) -> list[np.ndarray]:\n",
    "    out = []\n",
    "    for p in files:\n",
    "        y = load_series_from_window_file(p, value_col=value_col, require_metric=\"load_power\")\n",
    "        if y is None or len(y) < min_len:\n",
    "            continue\n",
    "        out.append(y.astype(np.float64))\n",
    "    return out\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Evaluation helpers\n",
    "# ----------------------------\n",
    "@torch.no_grad()\n",
    "def eval_teacher_forced_metrics(model: WaveNetCategorical, loader: DataLoader, cfg: Cfg, device: torch.device,max_batches: int = 200) -> dict:\n",
    "    model.eval()\n",
    "    rf = int(cfg.receptive_field)\n",
    "\n",
    "    centers = np.linspace(cfg.amp_min, cfg.amp_max, cfg.n_bins, dtype=np.float64)\n",
    "    centers_t = torch.tensor(centers, device=device, dtype=torch.float32)\n",
    "\n",
    "    y_true_all, y_pred_all = [], []\n",
    "\n",
    "    for bi, (xb, yb) in enumerate(loader):\n",
    "        if bi >= max_batches:\n",
    "            break\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        logits = model(xb)           # [B,K,T]\n",
    "        logits_v = logits[:, :, rf:] # [B,K,T-rf]\n",
    "        y_v = yb[:, rf:]             # [B,T-rf]\n",
    "\n",
    "        probs = torch.softmax(logits_v, dim=1)\n",
    "        y_pred = (probs * centers_t.view(1, -1, 1)).sum(dim=1)  # [B,T-rf]\n",
    "\n",
    "        y_true = mu_law_decode_np(\n",
    "            y_v.detach().cpu().numpy(),\n",
    "            mu=cfg.n_bins - 1,\n",
    "            amp_max=cfg.amp_max,\n",
    "        ).reshape(-1)\n",
    "\n",
    "        y_pred = y_pred.detach().cpu().numpy().reshape(-1)\n",
    "        y_true_all.append(y_true)\n",
    "        y_pred_all.append(y_pred)\n",
    "\n",
    "    if not y_true_all:\n",
    "        return {\"MSE\": float(\"nan\"), \"MAE\": float(\"nan\"), \"Corr\": float(\"nan\"), \"N\": 0.0}\n",
    "\n",
    "    return metrics_1d(np.concatenate(y_true_all), np.concatenate(y_pred_all))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_teacher_forced_ce(model: WaveNetCategorical, loader: DataLoader, cfg: Cfg, device: torch.device,\n",
    "                           max_batches: int = 200) -> float:\n",
    "    \"\"\"Compute teacher-forced cross-entropy (same objective as training) on a loader.\"\"\"\n",
    "    import torch.nn.functional as F\n",
    "    model.eval()\n",
    "    rf = int(cfg.receptive_field)\n",
    "    losses = []\n",
    "    for bi, (xb, yb) in enumerate(loader):\n",
    "        if bi >= max_batches:\n",
    "            break\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        logits = model(xb)                # [B,K,T]\n",
    "        logits_v = logits[:, :, rf:]\n",
    "        y_v = yb[:, rf:]\n",
    "        loss = F.cross_entropy(\n",
    "            logits_v.permute(0, 2, 1).reshape(-1, cfg.n_bins),\n",
    "            y_v.reshape(-1)\n",
    "        )\n",
    "        losses.append(float(loss.item()))\n",
    "    return float(np.mean(losses)) if losses else float(\"nan\")\n",
    "\n",
    "\n",
    "def load_best_last_ckpts(ckpt_dir: Path):\n",
    "    best_path = ckpt_dir / \"best.pt\"\n",
    "    last_path = ckpt_dir / \"last.pt\"\n",
    "    best = torch.load(best_path, map_location=\"cpu\", weights_only=False) if best_path.exists() else None\n",
    "    last = torch.load(last_path, map_location=\"cpu\", weights_only=False) if last_path.exists() else None\n",
    "    return best, last\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Time-series-safe k-fold splitter (blocked folds)\n",
    "# ----------------------------\n",
    "def blocked_kfold(files_sorted: list[Path], k: int) -> list[tuple[list[Path], list[Path]]]:\n",
    "    \"\"\"\n",
    "    Split ordered files into k contiguous blocks (validation blocks).\n",
    "    Train = all other blocks.\n",
    "    \"\"\"\n",
    "    n = len(files_sorted)\n",
    "    if n < k:\n",
    "        k = n\n",
    "    fold_sizes = [(n // k) + (1 if i < (n % k) else 0) for i in range(k)]\n",
    "    splits = []\n",
    "    start = 0\n",
    "    for fs in fold_sizes:\n",
    "        end = start + fs\n",
    "        val_files = files_sorted[start:end]\n",
    "        train_files = files_sorted[:start] + files_sorted[end:]\n",
    "        splits.append((train_files, val_files))\n",
    "        start = end\n",
    "    return splits\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# One fold run\n",
    "# ----------------------------\n",
    "def run_fold(months: int, fold_idx: int, train_files: list[Path], val_files: list[Path]) -> dict:\n",
    "    run_dir = RUNS_DIR / f\"{months}m\" / f\"fold_{fold_idx}\"\n",
    "    ckpt_dir = run_dir / \"checkpoints\"\n",
    "    plots_dir = run_dir / \"plots\"\n",
    "    plots_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    logger = setup_logger(run_dir, name=f\"wavenet_{months}m_fold{fold_idx}\")\n",
    "    cfg = Cfg()\n",
    "\n",
    "    dils = dilations_1s_context()\n",
    "    cfg.receptive_field = int(receptive_field(cfg.kernel_size, dils))\n",
    "    cfg.seq_len = cfg.receptive_field + 256\n",
    "    min_len = cfg.seq_len + 10 + int(MIN_LEN_PADDING)\n",
    "\n",
    "    logger.info(f\"months={months} fold={fold_idx} device={DEVICE}\")\n",
    "    logger.info(f\"seq_len={cfg.seq_len} RF={cfg.receptive_field}\")\n",
    "    logger.info(f\"train_files={len(train_files)} val_files={len(val_files)}\")\n",
    "\n",
    "    tr_list = files_to_series_list(train_files, min_len=min_len, value_col=VALUE_COL)\n",
    "    va_list = files_to_series_list(val_files,   min_len=min_len, value_col=VALUE_COL)\n",
    "\n",
    "    if not tr_list or not va_list:\n",
    "        raise RuntimeError(f\"Fold {fold_idx} has insufficient usable series.\")\n",
    "\n",
    "    train_ds = RandomWaveNetSegments(\n",
    "        epochs_1d=tr_list,\n",
    "        seq_len=cfg.seq_len,\n",
    "        n_samples=cfg.train_samples_per_epoch,\n",
    "        amp_min=cfg.amp_min, amp_max=cfg.amp_max, n_bins=cfg.n_bins,\n",
    "        rng=np.random.default_rng(1),\n",
    "    )\n",
    "\n",
    "    # fixed validation pairs for stability\n",
    "    val_ds_tmp = RandomWaveNetSegments(\n",
    "        epochs_1d=va_list,\n",
    "        seq_len=cfg.seq_len,\n",
    "        n_samples=cfg.val_samples_fixed,\n",
    "        amp_min=cfg.amp_min, amp_max=cfg.amp_max, n_bins=cfg.n_bins,\n",
    "        rng=np.random.default_rng(2),\n",
    "    )\n",
    "    fixed_pairs = list(val_ds_tmp.pairs)\n",
    "    val_ds = RandomWaveNetSegments(\n",
    "        epochs_1d=va_list,\n",
    "        seq_len=cfg.seq_len,\n",
    "        n_samples=len(fixed_pairs),\n",
    "        amp_min=cfg.amp_min, amp_max=cfg.amp_max, n_bins=cfg.n_bins,\n",
    "        rng=np.random.default_rng(2),\n",
    "        fixed_pairs=fixed_pairs,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False, num_workers=0, drop_last=False)\n",
    "\n",
    "    # Optional: create a small train loader for CE comparison (overfitting gap)\n",
    "    train_ds_eval_tmp = RandomWaveNetSegments(\n",
    "        epochs_1d=tr_list,\n",
    "        seq_len=cfg.seq_len,\n",
    "        n_samples=min(2000, cfg.train_samples_per_epoch),\n",
    "        amp_min=cfg.amp_min, amp_max=cfg.amp_max, n_bins=cfg.n_bins,\n",
    "        rng=np.random.default_rng(3),\n",
    "    )\n",
    "    train_loader_eval = DataLoader(train_ds_eval_tmp, batch_size=cfg.batch_size, shuffle=False, num_workers=0, drop_last=False)\n",
    "\n",
    "    model = WaveNetCategorical(n_bins=cfg.n_bins, n_filters=cfg.n_filters, kernel_size=cfg.kernel_size)\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    model = train_model(\n",
    "        model=model,\n",
    "        train_ds=train_ds,\n",
    "        val_loader=val_loader,\n",
    "        rf=cfg.receptive_field,\n",
    "        cfg=cfg,\n",
    "        device=DEVICE,\n",
    "        logger=logger,\n",
    "        wb_run=None,\n",
    "        ckpt_dir=ckpt_dir,\n",
    "    )\n",
    "    train_time = time.perf_counter() - t0\n",
    "\n",
    "    # Best/last checkpoint info\n",
    "    best, last = load_best_last_ckpts(ckpt_dir)\n",
    "    best_epoch = int(best.get(\"best_epoch\", best.get(\"epoch\", -1))) if best else -1\n",
    "    best_val_ce = float(best.get(\"best_val\", np.nan)) if best else float(\"nan\")\n",
    "    last_epoch = int(last.get(\"epoch\", -1)) if last else -1\n",
    "\n",
    "    # Evaluate metrics on BEST model (train_model reloads best.pt before returning)\n",
    "    val_m = eval_teacher_forced_metrics(model, val_loader, cfg, DEVICE, max_batches=250)\n",
    "    val_ce_tf = eval_teacher_forced_ce(model, val_loader, cfg, DEVICE, max_batches=250)\n",
    "    tr_ce_tf  = eval_teacher_forced_ce(model, train_loader_eval, cfg, DEVICE, max_batches=250)\n",
    "\n",
    "    # Overfitting signals\n",
    "    ce_gap = tr_ce_tf - val_ce_tf                # negative is good; positive suggests overfitting\n",
    "    trained_past_best = (last_epoch > best_epoch) if (best_epoch >= 0 and last_epoch >= 0) else False\n",
    "\n",
    "    # Quick plots: CE gap bar + a tiny summary plot\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.bar([\"train_CE\", \"val_CE\"], [tr_ce_tf, val_ce_tf])\n",
    "    plt.title(f\"{months}m fold {fold_idx} CE (teacher-forced)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plots_dir / \"ce_train_vs_val.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    # Save fold summary\n",
    "    summary = {\n",
    "        \"months\": months,\n",
    "        \"fold\": fold_idx,\n",
    "        \"train_files\": len(train_files),\n",
    "        \"val_files\": len(val_files),\n",
    "        \"train_time_s\": round(train_time, 2),\n",
    "        \"best_epoch\": best_epoch,\n",
    "        \"last_epoch\": last_epoch,\n",
    "        \"best_val_ce\": best_val_ce,\n",
    "        \"train_ce_teacher_forced\": tr_ce_tf,\n",
    "        \"val_ce_teacher_forced\": val_ce_tf,\n",
    "        \"ce_gap_train_minus_val\": ce_gap,\n",
    "        \"trained_past_best\": trained_past_best,\n",
    "        \"val_metrics_1d\": val_m,\n",
    "    }\n",
    "    (run_dir / \"fold_summary.json\").write_text(json.dumps(summary, indent=2))\n",
    "    logger.info(f\"FOLD SUMMARY: {summary}\")\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Run k-fold for one dataset\n",
    "# ----------------------------\n",
    "def run_kfold_for_months(months: int) -> dict:\n",
    "    ds_dir = WINDOW_ROOT / f\"{months}m\"\n",
    "    all_files = iter_window_files(ds_dir)\n",
    "\n",
    "    # Sort deterministically (time-safe-ish): by filename then path\n",
    "    # (your window naming included dates; if so, this is good. Otherwise it’s still consistent.)\n",
    "    all_files = sorted(all_files, key=lambda p: str(p))\n",
    "\n",
    "    if len(all_files) < 2:\n",
    "        raise RuntimeError(f\"Not enough window files in {ds_dir}\")\n",
    "\n",
    "    splits = blocked_kfold(all_files, K_FOLDS)\n",
    "\n",
    "    fold_summaries = []\n",
    "    for fi, (train_files, val_files) in enumerate(splits, start=1):\n",
    "        fold_summaries.append(run_fold(months, fi, train_files, val_files))\n",
    "\n",
    "    # Aggregate results\n",
    "    maes = [fs[\"val_metrics_1d\"][\"MAE\"] for fs in fold_summaries]\n",
    "    mses = [fs[\"val_metrics_1d\"][\"MSE\"] for fs in fold_summaries]\n",
    "    cors = [fs[\"val_metrics_1d\"][\"Corr\"] for fs in fold_summaries]\n",
    "    gaps = [fs[\"ce_gap_train_minus_val\"] for fs in fold_summaries]\n",
    "\n",
    "    agg = {\n",
    "        \"months\": months,\n",
    "        \"k_folds\": len(fold_summaries),\n",
    "        \"MAE_mean\": float(np.nanmean(maes)),\n",
    "        \"MAE_std\": float(np.nanstd(maes)),\n",
    "        \"MSE_mean\": float(np.nanmean(mses)),\n",
    "        \"MSE_std\": float(np.nanstd(mses)),\n",
    "        \"Corr_mean\": float(np.nanmean(cors)),\n",
    "        \"Corr_std\": float(np.nanstd(cors)),\n",
    "        \"CE_gap_mean\": float(np.nanmean(gaps)),\n",
    "        \"CE_gap_std\": float(np.nanstd(gaps)),\n",
    "        \"overfit_risk_flag\": bool(np.nanmean(gaps) > 0.0),  # avg train_ce > val_ce is a red flag\n",
    "    }\n",
    "\n",
    "    # Save + plot distribution of MAE across folds\n",
    "    out_dir = RUNS_DIR / f\"{months}m\"\n",
    "    (out_dir / \"kfold_aggregate.json\").write_text(json.dumps(agg, indent=2))\n",
    "\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.plot(range(1, len(maes) + 1), maes, marker=\"o\")\n",
    "    plt.xlabel(\"fold\")\n",
    "    plt.ylabel(\"VAL MAE (metrics_1d)\")\n",
    "    plt.title(f\"{months}m dataset: MAE across folds\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_dir / \"kfold_mae_across_folds.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    return agg\n",
    "\n",
    "\n",
    "def main():\n",
    "    RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    aggs = []\n",
    "    for months in WINDOW_MONTHS_LIST:\n",
    "        aggs.append(run_kfold_for_months(months))\n",
    "\n",
    "    # Single-line final report\n",
    "    # Example: 12m(MAE=0.123±0.01,gap=0.02±0.01,overfit=Y) | 6m(...) | 3m(...)\n",
    "    parts = []\n",
    "    for a in aggs:\n",
    "        parts.append(\n",
    "            f\"{a['months']}m(\"\n",
    "            f\"MAE={a['MAE_mean']:.4f}±{a['MAE_std']:.4f},\"\n",
    "            f\"Corr={a['Corr_mean']:.3f}±{a['Corr_std']:.3f},\"\n",
    "            f\"gap={a['CE_gap_mean']:.4f}±{a['CE_gap_std']:.4f},\"\n",
    "            f\"overfit={'Y' if a['overfit_risk_flag'] else 'N'})\"\n",
    "        )\n",
    "    print(\" | \".join(parts))\n",
    "\n",
    "    # Comparison plot: MAE_mean vs months\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    xs = [a[\"months\"] for a in aggs]\n",
    "    ys = [a[\"MAE_mean\"] for a in aggs]\n",
    "    plt.plot(xs, ys, marker=\"o\")\n",
    "    plt.xlabel(\"dataset (months)\")\n",
    "    plt.ylabel(\"K-fold mean VAL MAE\")\n",
    "    plt.title(\"K-fold validation MAE vs dataset window length\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RUNS_DIR / \"compare_kfold_mae.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"[DONE] Outputs saved under: {RUNS_DIR.resolve()}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "        main()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-22 01:07:51,777] INFO - months=3 fold=1 device=cpu\n",
      "[2026-02-22 01:07:51,778] INFO - seq_len=1256 RF=1000\n",
      "[2026-02-22 01:07:51,778] INFO - train_files=628 val_files=315\n",
      "[2026-02-22 01:10:48,743] INFO - epoch 001/200 | train_ce=4.846106e+00 | val_ce=4.334502e+00 | lr=1.000e-03 | dt=30.85s\n",
      "[2026-02-22 01:11:19,961] INFO - epoch 002/200 | train_ce=3.872791e+00 | val_ce=3.543097e+00 | lr=9.900e-04 | dt=31.17s\n",
      "[2026-02-22 01:11:49,376] INFO - epoch 003/200 | train_ce=3.158140e+00 | val_ce=3.113452e+00 | lr=9.801e-04 | dt=29.39s\n",
      "[2026-02-22 01:12:26,147] INFO - epoch 004/200 | train_ce=2.881385e+00 | val_ce=2.991494e+00 | lr=9.703e-04 | dt=36.73s\n",
      "[2026-02-22 01:13:31,074] INFO - epoch 005/200 | train_ce=2.786013e+00 | val_ce=2.911303e+00 | lr=9.606e-04 | dt=64.90s\n",
      "[2026-02-22 01:14:44,000] INFO - epoch 006/200 | train_ce=2.683351e+00 | val_ce=2.872121e+00 | lr=9.510e-04 | dt=72.88s\n",
      "[2026-02-22 01:15:46,598] INFO - epoch 007/200 | train_ce=2.688212e+00 | val_ce=2.845222e+00 | lr=9.415e-04 | dt=62.57s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 456\u001B[39m\n\u001B[32m    452\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m[DONE] Outputs saved under: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mRUNS_DIR.resolve()\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m    455\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[34m__name__\u001B[39m == \u001B[33m\"\u001B[39m\u001B[33m__main__\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m456\u001B[39m         \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 425\u001B[39m, in \u001B[36mmain\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    423\u001B[39m aggs = []\n\u001B[32m    424\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m months \u001B[38;5;129;01min\u001B[39;00m WINDOW_MONTHS_LIST:\n\u001B[32m--> \u001B[39m\u001B[32m425\u001B[39m     aggs.append(\u001B[43mrun_kfold_for_months\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmonths\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m    427\u001B[39m \u001B[38;5;66;03m# Single-line final report\u001B[39;00m\n\u001B[32m    428\u001B[39m \u001B[38;5;66;03m# Example: 12m(MAE=0.123±0.01,gap=0.02±0.01,overfit=Y) | 6m(...) | 3m(...)\u001B[39;00m\n\u001B[32m    429\u001B[39m parts = []\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 382\u001B[39m, in \u001B[36mrun_kfold_for_months\u001B[39m\u001B[34m(months)\u001B[39m\n\u001B[32m    380\u001B[39m fold_summaries = []\n\u001B[32m    381\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m fi, (train_files, val_files) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(splits, start=\u001B[32m1\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m382\u001B[39m     fold_summaries.append(\u001B[43mrun_fold\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmonths\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_files\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_files\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m    384\u001B[39m \u001B[38;5;66;03m# Aggregate results\u001B[39;00m\n\u001B[32m    385\u001B[39m maes = [fs[\u001B[33m\"\u001B[39m\u001B[33mval_metrics_1d\u001B[39m\u001B[33m\"\u001B[39m][\u001B[33m\"\u001B[39m\u001B[33mMAE\u001B[39m\u001B[33m\"\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m fs \u001B[38;5;129;01min\u001B[39;00m fold_summaries]\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 306\u001B[39m, in \u001B[36mrun_fold\u001B[39m\u001B[34m(months, fold_idx, train_files, val_files)\u001B[39m\n\u001B[32m    303\u001B[39m model = WaveNetCategorical(n_bins=cfg.n_bins, n_filters=cfg.n_filters, kernel_size=cfg.kernel_size)\n\u001B[32m    305\u001B[39m t0 = time.perf_counter()\n\u001B[32m--> \u001B[39m\u001B[32m306\u001B[39m model = \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    307\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    308\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_ds\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrain_ds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    309\u001B[39m \u001B[43m    \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m=\u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    310\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrf\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcfg\u001B[49m\u001B[43m.\u001B[49m\u001B[43mreceptive_field\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    311\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcfg\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcfg\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    312\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mDEVICE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    313\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlogger\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlogger\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    314\u001B[39m \u001B[43m    \u001B[49m\u001B[43mwb_run\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    315\u001B[39m \u001B[43m    \u001B[49m\u001B[43mckpt_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mckpt_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    316\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    317\u001B[39m train_time = time.perf_counter() - t0\n\u001B[32m    319\u001B[39m \u001B[38;5;66;03m# Best/last checkpoint info\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Coding\\HackEurope\\hackathon_IMAC\\core\\training.py:80\u001B[39m, in \u001B[36mtrain_model\u001B[39m\u001B[34m(model, train_ds, val_loader, rf, cfg, device, logger, wb_run, ckpt_dir)\u001B[39m\n\u001B[32m     77\u001B[39m y_v = yb[:, rf:]\n\u001B[32m     79\u001B[39m loss = F.cross_entropy(logits_v.permute(\u001B[32m0\u001B[39m, \u001B[32m2\u001B[39m, \u001B[32m1\u001B[39m).reshape(-\u001B[32m1\u001B[39m, cfg.n_bins), y_v.reshape(-\u001B[32m1\u001B[39m))\n\u001B[32m---> \u001B[39m\u001B[32m80\u001B[39m \u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     81\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m cfg.grad_clip \u001B[38;5;129;01mand\u001B[39;00m cfg.grad_clip > \u001B[32m0\u001B[39m:\n\u001B[32m     82\u001B[39m     torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\HackEurope\\.venv\\Lib\\site-packages\\torch\\_tensor.py:630\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    620\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    621\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    622\u001B[39m         Tensor.backward,\n\u001B[32m    623\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    628\u001B[39m         inputs=inputs,\n\u001B[32m    629\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m630\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    631\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    632\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\HackEurope\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:364\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    359\u001B[39m     retain_graph = create_graph\n\u001B[32m    361\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    362\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    363\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m364\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    365\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    366\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    367\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    368\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    369\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_tuple\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    370\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    371\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    372\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\HackEurope\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:865\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    863\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    864\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m865\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    866\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    867\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    868\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    869\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

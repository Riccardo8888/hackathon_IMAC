{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8788855",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader, Dataset\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwavenet_anna\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     42\u001b[39m     EEGWaveNetCategorical,\n\u001b[32m     43\u001b[39m     RandomWaveNetSegments,\n\u001b[32m     44\u001b[39m     dilations_1s_context,\n\u001b[32m     45\u001b[39m     receptive_field,\n\u001b[32m     46\u001b[39m     mu_law_decode_np,\n\u001b[32m     47\u001b[39m     mu_law_encode_np,\n\u001b[32m     48\u001b[39m )\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Global settings\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m     53\u001b[39m DEVICE = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricca\\hackathon_IMAC\\wavenet_anna.py:10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01margparse\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logger\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\__init__.py:70\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# `_distributor_init` allows distributors to run custom init code.\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# For instance, for the Windows wheel, this is used to pre-load the\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# vcomp shared library runtime for OpenMP embedded in the sklearn/.libs\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __check_build, _distributor_init  \u001b[38;5;66;03m# noqa: E402 F401\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     73\u001b[39m _submodules = [\n\u001b[32m     74\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcalibration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     75\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcluster\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    111\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcompose\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    112\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1176\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1138\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1078\u001b[39m, in \u001b[36m_find_spec\u001b[39m\u001b[34m(name, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1507\u001b[39m, in \u001b[36mfind_spec\u001b[39m\u001b[34m(cls, fullname, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1479\u001b[39m, in \u001b[36m_get_spec\u001b[39m\u001b[34m(cls, fullname, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1648\u001b[39m, in \u001b[36mfind_spec\u001b[39m\u001b[34m(self, fullname, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:161\u001b[39m, in \u001b[36m_path_isfile\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:153\u001b[39m, in \u001b[36m_path_is_mode_type\u001b[39m\u001b[34m(path, mode)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:147\u001b[39m, in \u001b[36m_path_stat\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "wavenet_overfit_holdout.py\n",
    "\n",
    "Removes k-fold cross validation.\n",
    "Uses:\n",
    "  (A) time-series-safe HOLDOUT split (contiguous blocks of window-files)\n",
    "  (B) explicit overfitting test on a tiny fixed dataset (no resampling)\n",
    "\n",
    "Datasets expected:\n",
    "  data_windows_fast/12m , /6m , /3m\n",
    "Window files: .parquet or .csv(.gz)\n",
    "Must contain 'utc' (optional) and VALUE_COL (default 'max').\n",
    "\n",
    "Outputs:\n",
    "  trained_wavenet_holdout_tests/<months>m/\n",
    "    - overfit_test_summary.json\n",
    "    - holdout_summary.json\n",
    "    - checkpoints_overfit/{best.pt,last.pt}\n",
    "    - checkpoints_holdout/{best.pt,last.pt}\n",
    "    - plots/*.png\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from wavenet_anna import (\n",
    "    EEGWaveNetCategorical,\n",
    "    RandomWaveNetSegments,\n",
    "    dilations_1s_context,\n",
    "    receptive_field,\n",
    "    mu_law_decode_np,\n",
    "    mu_law_encode_np,\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Global settings\n",
    "# ----------------------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "WINDOW_ROOT = Path(\"data_windows_fast\")\n",
    "WINDOW_MONTHS_LIST = (12, 6, 3)\n",
    "VALUE_COL = \"max\"\n",
    "RUNS_DIR = Path(\"trained_wavenet_holdout_tests\")\n",
    "RECURSIVE = True\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    n_bins: int = 256\n",
    "    kernel_size: int = 2\n",
    "    n_filters: int = 16\n",
    "\n",
    "    lr: float = 1e-3\n",
    "    lr_decay_gamma: float = 0.99\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 200\n",
    "    train_samples_per_epoch: int = 6000\n",
    "    val_samples_fixed: int = 1500\n",
    "    early_stop_patience: int = 10\n",
    "    grad_clip: float = 1.0\n",
    "\n",
    "    # mu-law range (set automatically from train unless overridden)\n",
    "    amp_min: float = -3.0\n",
    "    amp_max: float = 3.0\n",
    "\n",
    "    receptive_field: int = 1024\n",
    "    seq_len: int = 0\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Logging\n",
    "# ----------------------------\n",
    "def setup_logger(out_dir: Path, name: str) -> logging.Logger:\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    lg = logging.getLogger(name)\n",
    "    lg.setLevel(logging.INFO)\n",
    "    lg.handlers.clear()\n",
    "    fmt = logging.Formatter(\"[%(asctime)s] %(levelname)s - %(message)s\")\n",
    "\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setFormatter(fmt)\n",
    "    lg.addHandler(ch)\n",
    "\n",
    "    fh = logging.FileHandler(out_dir / \"run.log\")\n",
    "    fh.setFormatter(fmt)\n",
    "    lg.addHandler(fh)\n",
    "    return lg\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Window loading helpers\n",
    "# ----------------------------\n",
    "def iter_window_files(ds_dir: Path) -> List[Path]:\n",
    "    pats = [\"**/*.parquet\", \"**/*.csv.gz\", \"**/*.csv\"] if RECURSIVE else [\"*.parquet\", \"*.csv.gz\", \"*.csv\"]\n",
    "    files: List[Path] = []\n",
    "    for pat in pats:\n",
    "        files.extend(ds_dir.glob(pat))\n",
    "    return sorted(set(files))\n",
    "\n",
    "def read_window_file(path: Path) -> pd.DataFrame:\n",
    "    if path.suffix.lower() == \".parquet\":\n",
    "        return pd.read_parquet(path)\n",
    "    if path.name.lower().endswith(\".csv.gz\"):\n",
    "        return pd.read_csv(path, compression=\"gzip\")\n",
    "    if path.suffix.lower() == \".csv\":\n",
    "        return pd.read_csv(path)\n",
    "    raise ValueError(f\"Unsupported window file: {path}\")\n",
    "\n",
    "def load_series_from_window_file(path: Path, value_col: str = \"max\") -> Optional[np.ndarray]:\n",
    "    df = read_window_file(path)\n",
    "    if \"utc\" in df.columns:\n",
    "        df[\"utc\"] = pd.to_datetime(df[\"utc\"], errors=\"coerce\")\n",
    "        df = df.dropna(subset=[\"utc\"]).sort_values(\"utc\")\n",
    "    if value_col not in df.columns:\n",
    "        return None\n",
    "    y = pd.to_numeric(df[value_col], errors=\"coerce\").dropna().to_numpy(dtype=np.float64)\n",
    "    if y.size < 10:\n",
    "        return None\n",
    "    return y\n",
    "\n",
    "def files_to_series_list(files: List[Path], min_len: int, value_col: str) -> List[np.ndarray]:\n",
    "    out: List[np.ndarray] = []\n",
    "    for p in files:\n",
    "        y = load_series_from_window_file(p, value_col=value_col)\n",
    "        if y is None or len(y) < min_len:\n",
    "            continue\n",
    "        out.append(y.astype(np.float64))\n",
    "    return out\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Time-series-safe holdout split (by contiguous file blocks)\n",
    "# ----------------------------\n",
    "def holdout_split_files(files_sorted: List[Path], train_frac: float, val_frac: float) -> Tuple[List[Path], List[Path], List[Path]]:\n",
    "    n = len(files_sorted)\n",
    "    n_train = int(round(n * train_frac))\n",
    "    n_val = int(round(n * val_frac))\n",
    "    n_train = max(1, min(n_train, n - 2))\n",
    "    n_val = max(1, min(n_val, n - n_train - 1))\n",
    "    train_files = files_sorted[:n_train]\n",
    "    val_files = files_sorted[n_train:n_train + n_val]\n",
    "    test_files = files_sorted[n_train + n_val:]\n",
    "    return train_files, val_files, test_files\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Fixed dataset for overfitting test (NO resampling)\n",
    "# ----------------------------\n",
    "class FixedWaveNetSegments(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        epochs_1d: List[np.ndarray],\n",
    "        seq_len: int,\n",
    "        amp_min: float,\n",
    "        amp_max: float,\n",
    "        n_bins: int,\n",
    "        fixed_pairs: List[Tuple[int, int]],\n",
    "    ):\n",
    "        self.epochs = epochs_1d\n",
    "        self.seq_len = int(seq_len)\n",
    "        self.amp_min = float(amp_min)\n",
    "        self.amp_max = float(amp_max)\n",
    "        self.n_bins = int(n_bins)\n",
    "        self.pairs = list(fixed_pairs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        e, s = self.pairs[idx]\n",
    "        seg = self.epochs[e][s:s + self.seq_len].astype(np.float64)\n",
    "        y = mu_law_encode_np(seg, mu=self.n_bins - 1, amp_max=self.amp_max)\n",
    "        x_in = np.empty_like(y)\n",
    "        x_in[0] = 0\n",
    "        x_in[1:] = y[:-1]\n",
    "        return torch.from_numpy(x_in.astype(np.int64)), torch.from_numpy(y.astype(np.int64))\n",
    "\n",
    "\n",
    "def sample_fixed_pairs(\n",
    "    epochs_1d: List[np.ndarray],\n",
    "    seq_len: int,\n",
    "    n_samples: int,\n",
    "    amp_min: float,\n",
    "    amp_max: float,\n",
    "    n_bins: int,\n",
    "    seed: int = 0,\n",
    ") -> List[Tuple[int, int]]:\n",
    "    tmp = RandomWaveNetSegments(\n",
    "        epochs_1d=epochs_1d,\n",
    "        seq_len=seq_len,\n",
    "        n_samples=n_samples,\n",
    "        amp_min=amp_min,\n",
    "        amp_max=amp_max,\n",
    "        n_bins=n_bins,\n",
    "        rng=np.random.default_rng(seed),\n",
    "    )\n",
    "    return list(tmp.pairs)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Training loop with history (so you can see overfit curves)\n",
    "# ----------------------------\n",
    "def train_with_history(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    rf: int,\n",
    "    cfg: Cfg,\n",
    "    device: torch.device,\n",
    "    logger: logging.Logger,\n",
    "    ckpt_dir: Path,\n",
    ") -> dict:\n",
    "    ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "    best_path = ckpt_dir / \"best.pt\"\n",
    "    last_path = ckpt_dir / \"last.pt\"\n",
    "\n",
    "    model.to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=cfg.lr)\n",
    "    sched = torch.optim.lr_scheduler.ExponentialLR(opt, gamma=float(cfg.lr_decay_gamma))\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    best_epoch = 0\n",
    "    bad = 0\n",
    "\n",
    "    hist = {\"epoch\": [], \"train_ce\": [], \"val_ce\": [], \"lr\": []}\n",
    "\n",
    "    for ep in range(1, cfg.epochs + 1):\n",
    "        t0 = time.perf_counter()\n",
    "        model.train()\n",
    "        tr_losses = []\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            logits = model(xb)\n",
    "            logits_v = logits[:, :, rf:]\n",
    "            y_v = yb[:, rf:]\n",
    "\n",
    "            loss = F.cross_entropy(logits_v.permute(0, 2, 1).reshape(-1, cfg.n_bins), y_v.reshape(-1))\n",
    "            loss.backward()\n",
    "            if cfg.grad_clip and cfg.grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "            opt.step()\n",
    "            tr_losses.append(float(loss.item()))\n",
    "\n",
    "        model.eval()\n",
    "        va_losses = []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "                logits = model(xb)\n",
    "                logits_v = logits[:, :, rf:]\n",
    "                y_v = yb[:, rf:]\n",
    "                loss = F.cross_entropy(logits_v.permute(0, 2, 1).reshape(-1, cfg.n_bins), y_v.reshape(-1))\n",
    "                va_losses.append(float(loss.item()))\n",
    "\n",
    "        tr = float(np.mean(tr_losses)) if tr_losses else float(\"inf\")\n",
    "        va = float(np.mean(va_losses)) if va_losses else float(\"inf\")\n",
    "        lr_now = float(opt.param_groups[0][\"lr\"])\n",
    "        dt = time.perf_counter() - t0\n",
    "\n",
    "        hist[\"epoch\"].append(ep)\n",
    "        hist[\"train_ce\"].append(tr)\n",
    "        hist[\"val_ce\"].append(va)\n",
    "        hist[\"lr\"].append(lr_now)\n",
    "\n",
    "        logger.info(f\"epoch {ep:03d}/{cfg.epochs} | train_ce={tr:.6e} | val_ce={va:.6e} | lr={lr_now:.3e} | dt={dt:.2f}s\")\n",
    "\n",
    "        improved = (va + 1e-12) < best_val\n",
    "        if improved:\n",
    "            best_val = va\n",
    "            best_epoch = ep\n",
    "            bad = 0\n",
    "            torch.save(\n",
    "                {\"epoch\": ep, \"best_val\": best_val, \"best_epoch\": best_epoch, \"model\": model.state_dict(),\n",
    "                 \"opt\": opt.state_dict(), \"sched\": sched.state_dict(), \"bad\": bad},\n",
    "                best_path,\n",
    "            )\n",
    "        else:\n",
    "            bad += 1\n",
    "\n",
    "        sched.step()\n",
    "\n",
    "        torch.save(\n",
    "            {\"epoch\": ep, \"best_val\": best_val, \"best_epoch\": best_epoch, \"model\": model.state_dict(),\n",
    "             \"opt\": opt.state_dict(), \"sched\": sched.state_dict(), \"bad\": bad},\n",
    "            last_path,\n",
    "        )\n",
    "\n",
    "        if bad >= int(cfg.early_stop_patience):\n",
    "            logger.info(f\"early stop @ epoch={ep} (patience={cfg.early_stop_patience}) best_epoch={best_epoch} best_val={best_val:.6e}\")\n",
    "            break\n",
    "\n",
    "    # reload best\n",
    "    if best_path.exists():\n",
    "        best = torch.load(best_path, map_location=\"cpu\", weights_only=False)\n",
    "        model.load_state_dict(best[\"model\"])\n",
    "\n",
    "    return {\"history\": hist, \"best_val_ce\": float(best_val), \"best_epoch\": int(best_epoch)}\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Evaluation helpers (teacher-forced CE + expected-value metrics)\n",
    "# ----------------------------\n",
    "@torch.no_grad()\n",
    "def eval_teacher_forced_ce(model: EEGWaveNetCategorical, loader: DataLoader, cfg: Cfg, device: torch.device, max_batches: int = 300) -> float:\n",
    "    model.eval()\n",
    "    rf = int(cfg.receptive_field)\n",
    "    losses = []\n",
    "    for bi, (xb, yb) in enumerate(loader):\n",
    "        if bi >= max_batches:\n",
    "            break\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        logits = model(xb)\n",
    "        logits_v = logits[:, :, rf:]\n",
    "        y_v = yb[:, rf:]\n",
    "        loss = F.cross_entropy(logits_v.permute(0, 2, 1).reshape(-1, cfg.n_bins), y_v.reshape(-1))\n",
    "        losses.append(float(loss.item()))\n",
    "    return float(np.mean(losses)) if losses else float(\"nan\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_teacher_forced_metrics(model: EEGWaveNetCategorical, loader: DataLoader, cfg: Cfg, device: torch.device, max_batches: int = 300) -> dict:\n",
    "    \"\"\"\n",
    "    Computes MSE/MAE/Corr on expected amplitude under the predicted distribution (teacher forced).\n",
    "    Uses TRUE mu-law decoded bin values (not linear centers).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    rf = int(cfg.receptive_field)\n",
    "\n",
    "    bin_vals = mu_law_decode_np(np.arange(cfg.n_bins), mu=cfg.n_bins - 1, amp_max=cfg.amp_max).astype(np.float64)\n",
    "    bin_vals_t = torch.tensor(bin_vals, device=device, dtype=torch.float32)  # (K,)\n",
    "\n",
    "    y_true_all, y_pred_all = [], []\n",
    "\n",
    "    for bi, (xb, yb) in enumerate(loader):\n",
    "        if bi >= max_batches:\n",
    "            break\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        logits = model(xb)              # [B,K,T]\n",
    "        logits_v = logits[:, :, rf:]    # [B,K,T-rf]\n",
    "        y_v = yb[:, rf:]                # [B,T-rf]\n",
    "\n",
    "        probs = torch.softmax(logits_v, dim=1)                     # [B,K,T-rf]\n",
    "        y_pred = (probs * bin_vals_t.view(1, -1, 1)).sum(dim=1)    # [B,T-rf]\n",
    "\n",
    "        y_true = mu_law_decode_np(\n",
    "            y_v.detach().cpu().numpy(),\n",
    "            mu=cfg.n_bins - 1,\n",
    "            amp_max=cfg.amp_max,\n",
    "        ).reshape(-1)\n",
    "\n",
    "        y_pred = y_pred.detach().cpu().numpy().reshape(-1)\n",
    "\n",
    "        y_true_all.append(y_true)\n",
    "        y_pred_all.append(y_pred)\n",
    "\n",
    "    if not y_true_all:\n",
    "        return {\"MSE\": float(\"nan\"), \"MAE\": float(\"nan\"), \"Corr\": float(\"nan\"), \"N\": 0.0}\n",
    "\n",
    "    yt = np.concatenate(y_true_all)\n",
    "    yp = np.concatenate(y_pred_all)\n",
    "    mse = float(np.mean((yt - yp) ** 2))\n",
    "    mae = float(np.mean(np.abs(yt - yp)))\n",
    "    corr = float(np.corrcoef(yt, yp)[0, 1]) if (yt.size > 1 and np.std(yt) > 0 and np.std(yp) > 0) else float(\"nan\")\n",
    "    return {\"MSE\": mse, \"MAE\": mae, \"Corr\": corr, \"N\": float(yt.size)}\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Amp range helper (important)\n",
    "# ----------------------------\n",
    "def auto_amp_from_train(tr_list: List[np.ndarray], pct: float = 99.9, sample_per_series: int = 20000) -> float:\n",
    "    xs = []\n",
    "    for ep in tr_list:\n",
    "        if len(ep) == 0:\n",
    "            continue\n",
    "        take = min(len(ep), sample_per_series)\n",
    "        xs.append(np.abs(ep[:take]))\n",
    "    xcat = np.concatenate(xs) if xs else np.array([1.0], dtype=np.float64)\n",
    "    return float(np.percentile(xcat, pct))\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Main per-month runner\n",
    "# ----------------------------\n",
    "def run_one_months(\n",
    "    months: int,\n",
    "    train_frac: float = 0.70,\n",
    "    val_frac: float = 0.15,\n",
    "    auto_amp_pct: float = 99.9,\n",
    "    overfit_n_files: int = 2,\n",
    "    overfit_fixed_segments: int = 1500,\n",
    "    overfit_epochs: int = 250,\n",
    "    holdout_epochs: int = 200,\n",
    "):\n",
    "    run_dir = RUNS_DIR / f\"{months}m\"\n",
    "    ckpt_overfit = run_dir / \"checkpoints_overfit\"\n",
    "    ckpt_holdout = run_dir / \"checkpoints_holdout\"\n",
    "    plots_dir = run_dir / \"plots\"\n",
    "    plots_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    logger = setup_logger(run_dir, name=f\"wavenet_holdout_{months}m\")\n",
    "\n",
    "    # list & split files (contiguous blocks)\n",
    "    ds_dir = WINDOW_ROOT / f\"{months}m\"\n",
    "    all_files = iter_window_files(ds_dir)\n",
    "    all_files = sorted(all_files, key=lambda p: str(p))  # deterministic\n",
    "    if len(all_files) < 5:\n",
    "        raise RuntimeError(f\"Not enough window files in {ds_dir} (found {len(all_files)})\")\n",
    "\n",
    "    train_files, val_files, test_files = holdout_split_files(all_files, train_frac=train_frac, val_frac=val_frac)\n",
    "    logger.info(f\"[FILES] months={months} total={len(all_files)} train={len(train_files)} val={len(val_files)} test={len(test_files)}\")\n",
    "\n",
    "    cfg = Cfg()\n",
    "    dils = dilations_1s_context()\n",
    "    cfg.receptive_field = int(receptive_field(cfg.kernel_size, dils))\n",
    "    cfg.seq_len = cfg.receptive_field + 120\n",
    "    min_len = cfg.seq_len + 10\n",
    "\n",
    "    # load series\n",
    "    tr_list = files_to_series_list(train_files, min_len=min_len, value_col=VALUE_COL)\n",
    "    va_list = files_to_series_list(val_files,   min_len=min_len, value_col=VALUE_COL)\n",
    "    te_list = files_to_series_list(test_files,  min_len=min_len, value_col=VALUE_COL)\n",
    "    if not tr_list or not va_list or not te_list:\n",
    "        raise RuntimeError(f\"[DATA] insufficient usable series after filtering: train={len(tr_list)} val={len(va_list)} test={len(te_list)}\")\n",
    "\n",
    "    # auto amp range from TRAIN\n",
    "    amp = auto_amp_from_train(tr_list, pct=auto_amp_pct)\n",
    "    cfg.amp_max = amp\n",
    "    cfg.amp_min = -amp\n",
    "    logger.info(f\"[AMP] auto pct={auto_amp_pct} -> amp_max={cfg.amp_max:.6e} amp_min={cfg.amp_min:.6e}\")\n",
    "\n",
    "    logger.info(f\"[CFG] RF={cfg.receptive_field} seq_len={cfg.seq_len} device={DEVICE}\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # (B) Overfitting test: tiny fixed dataset\n",
    "    # ----------------------------\n",
    "    tiny_files = train_files[: max(1, min(overfit_n_files, len(train_files)))]\n",
    "    tiny_list = files_to_series_list(tiny_files, min_len=min_len, value_col=VALUE_COL)\n",
    "    if not tiny_list:\n",
    "        raise RuntimeError(\"[OVERFIT] tiny_list empty; increase overfit_n_files or reduce min_len\")\n",
    "\n",
    "    cfg_overfit = Cfg(**asdict(cfg))\n",
    "    cfg_overfit.epochs = int(overfit_epochs)\n",
    "    cfg_overfit.early_stop_patience = max(20, cfg_overfit.early_stop_patience)  # allow real overfit\n",
    "    cfg_overfit.train_samples_per_epoch = int(overfit_fixed_segments)\n",
    "    cfg_overfit.val_samples_fixed = int(overfit_fixed_segments)\n",
    "\n",
    "    # fixed pairs -> fixed dataset\n",
    "    fixed_pairs = sample_fixed_pairs(\n",
    "        epochs_1d=tiny_list,\n",
    "        seq_len=cfg_overfit.seq_len,\n",
    "        n_samples=cfg_overfit.train_samples_per_epoch,\n",
    "        amp_min=cfg_overfit.amp_min,\n",
    "        amp_max=cfg_overfit.amp_max,\n",
    "        n_bins=cfg_overfit.n_bins,\n",
    "        seed=123,\n",
    "    )\n",
    "    train_ds_fix = FixedWaveNetSegments(\n",
    "        epochs_1d=tiny_list,\n",
    "        seq_len=cfg_overfit.seq_len,\n",
    "        amp_min=cfg_overfit.amp_min,\n",
    "        amp_max=cfg_overfit.amp_max,\n",
    "        n_bins=cfg_overfit.n_bins,\n",
    "        fixed_pairs=fixed_pairs,\n",
    "    )\n",
    "    # for overfit test, val loader is SAME fixed dataset (measures memorization)\n",
    "    val_ds_fix = train_ds_fix\n",
    "\n",
    "    train_loader_fix = DataLoader(train_ds_fix, batch_size=cfg_overfit.batch_size, shuffle=True, num_workers=0, drop_last=True)\n",
    "    val_loader_fix   = DataLoader(val_ds_fix,   batch_size=cfg_overfit.batch_size, shuffle=False, num_workers=0, drop_last=False)\n",
    "\n",
    "    model_overfit = EEGWaveNetCategorical(n_bins=cfg_overfit.n_bins, n_filters=cfg_overfit.n_filters, kernel_size=cfg_overfit.kernel_size)\n",
    "    logger.info(f\"[OVERFIT] training on {len(tiny_files)} file(s), fixed_segments={len(train_ds_fix)} epochs={cfg_overfit.epochs}\")\n",
    "\n",
    "    out_overfit = train_with_history(\n",
    "        model=model_overfit,\n",
    "        train_loader=train_loader_fix,\n",
    "        val_loader=val_loader_fix,\n",
    "        rf=cfg_overfit.receptive_field,\n",
    "        cfg=cfg_overfit,\n",
    "        device=DEVICE,\n",
    "        logger=logger,\n",
    "        ckpt_dir=ckpt_overfit,\n",
    "    )\n",
    "\n",
    "    # compute CE on fixed set (train==val) and also on REAL holdout val\n",
    "    # create a small REAL holdout val loader for comparison\n",
    "    val_pairs = sample_fixed_pairs(va_list, cfg.seq_len, n_samples=min(1500, cfg.val_samples_fixed),\n",
    "                                   amp_min=cfg.amp_min, amp_max=cfg.amp_max, n_bins=cfg.n_bins, seed=456)\n",
    "    val_ds_real = FixedWaveNetSegments(va_list, cfg.seq_len, cfg.amp_min, cfg.amp_max, cfg.n_bins, val_pairs)\n",
    "    val_loader_real = DataLoader(val_ds_real, batch_size=cfg.batch_size, shuffle=False, num_workers=0, drop_last=False)\n",
    "\n",
    "    train_ce_fix = eval_teacher_forced_ce(model_overfit, train_loader_fix, cfg_overfit, DEVICE)\n",
    "    val_ce_fix   = eval_teacher_forced_ce(model_overfit, val_loader_fix,   cfg_overfit, DEVICE)\n",
    "    val_ce_real  = eval_teacher_forced_ce(model_overfit, val_loader_real,  cfg, DEVICE)\n",
    "\n",
    "    overfit_summary = {\n",
    "        \"months\": months,\n",
    "        \"tiny_files_used\": [str(p) for p in tiny_files],\n",
    "        \"fixed_segments\": int(len(train_ds_fix)),\n",
    "        \"best_epoch\": int(out_overfit[\"best_epoch\"]),\n",
    "        \"best_val_ce_fixedset\": float(out_overfit[\"best_val_ce\"]),\n",
    "        \"train_ce_fixedset\": float(train_ce_fix),\n",
    "        \"val_ce_fixedset\": float(val_ce_fix),\n",
    "        \"val_ce_real_holdout\": float(val_ce_real),\n",
    "        \"gap_fixed_train_minus_real_val\": float(train_ce_fix - val_ce_real),\n",
    "        \"history\": out_overfit[\"history\"],\n",
    "    }\n",
    "\n",
    "    (run_dir / \"overfit_test_summary.json\").write_text(json.dumps(overfit_summary, indent=2))\n",
    "\n",
    "    # plot CE curves for overfit\n",
    "    h = overfit_summary[\"history\"]\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.plot(h[\"epoch\"], h[\"train_ce\"], marker=\"o\", label=\"train_ce (fixed)\")\n",
    "    plt.plot(h[\"epoch\"], h[\"val_ce\"], marker=\"o\", label=\"val_ce (fixed)\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"cross-entropy\")\n",
    "    plt.title(f\"{months}m OVERFIT TEST (tiny fixed set)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plots_dir / \"overfit_ce_curves.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    logger.info(f\"[OVERFIT] train_ce_fixed={train_ce_fix:.4f} val_ce_fixed={val_ce_fix:.4f} val_ce_real={val_ce_real:.4f}\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # (A) Holdout training (normal training, but single split)\n",
    "    # ----------------------------\n",
    "    cfg_holdout = Cfg(**asdict(cfg))\n",
    "    cfg_holdout.epochs = int(holdout_epochs)\n",
    "\n",
    "    train_ds = RandomWaveNetSegments(\n",
    "        epochs_1d=tr_list,\n",
    "        seq_len=cfg_holdout.seq_len,\n",
    "        n_samples=cfg_holdout.train_samples_per_epoch,\n",
    "        amp_min=cfg_holdout.amp_min,\n",
    "        amp_max=cfg_holdout.amp_max,\n",
    "        n_bins=cfg_holdout.n_bins,\n",
    "        rng=np.random.default_rng(1),\n",
    "    )\n",
    "\n",
    "    # fixed val for stability\n",
    "    val_ds_tmp = RandomWaveNetSegments(\n",
    "        epochs_1d=va_list,\n",
    "        seq_len=cfg_holdout.seq_len,\n",
    "        n_samples=cfg_holdout.val_samples_fixed,\n",
    "        amp_min=cfg_holdout.amp_min,\n",
    "        amp_max=cfg_holdout.amp_max,\n",
    "        n_bins=cfg_holdout.n_bins,\n",
    "        rng=np.random.default_rng(2),\n",
    "    )\n",
    "    fixed_val_pairs = list(val_ds_tmp.pairs)\n",
    "    val_ds = FixedWaveNetSegments(\n",
    "        epochs_1d=va_list,\n",
    "        seq_len=cfg_holdout.seq_len,\n",
    "        amp_min=cfg_holdout.amp_min,\n",
    "        amp_max=cfg_holdout.amp_max,\n",
    "        n_bins=cfg_holdout.n_bins,\n",
    "        fixed_pairs=fixed_val_pairs,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=cfg_holdout.batch_size, shuffle=True, num_workers=0, drop_last=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=cfg_holdout.batch_size, shuffle=False, num_workers=0, drop_last=False)\n",
    "\n",
    "    model_holdout = EEGWaveNetCategorical(n_bins=cfg_holdout.n_bins, n_filters=cfg_holdout.n_filters, kernel_size=cfg_holdout.kernel_size)\n",
    "\n",
    "    logger.info(f\"[HOLDOUT] training: epochs={cfg_holdout.epochs} train_samples/epoch={cfg_holdout.train_samples_per_epoch} val_fixed={len(val_ds)}\")\n",
    "    out_holdout = train_with_history(\n",
    "        model=model_holdout,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        rf=cfg_holdout.receptive_field,\n",
    "        cfg=cfg_holdout,\n",
    "        device=DEVICE,\n",
    "        logger=logger,\n",
    "        ckpt_dir=ckpt_holdout,\n",
    "    )\n",
    "\n",
    "    # evaluate on val/test with teacher-forced metrics\n",
    "    val_ce = eval_teacher_forced_ce(model_holdout, val_loader, cfg_holdout, DEVICE)\n",
    "    val_m  = eval_teacher_forced_metrics(model_holdout, val_loader, cfg_holdout, DEVICE)\n",
    "\n",
    "    # make a fixed test loader\n",
    "    test_pairs = sample_fixed_pairs(te_list, cfg.seq_len, n_samples=min(1500, cfg.val_samples_fixed),\n",
    "                                    amp_min=cfg.amp_min, amp_max=cfg.amp_max, n_bins=cfg.n_bins, seed=789)\n",
    "    test_ds = FixedWaveNetSegments(te_list, cfg.seq_len, cfg.amp_min, cfg.amp_max, cfg.n_bins, test_pairs)\n",
    "    test_loader = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False, num_workers=0, drop_last=False)\n",
    "\n",
    "    test_ce = eval_teacher_forced_ce(model_holdout, test_loader, cfg_holdout, DEVICE)\n",
    "    test_m  = eval_teacher_forced_metrics(model_holdout, test_loader, cfg_holdout, DEVICE)\n",
    "\n",
    "    holdout_summary = {\n",
    "        \"months\": months,\n",
    "        \"files_total\": len(all_files),\n",
    "        \"files_train\": len(train_files),\n",
    "        \"files_val\": len(val_files),\n",
    "        \"files_test\": len(test_files),\n",
    "        \"amp_min\": cfg_holdout.amp_min,\n",
    "        \"amp_max\": cfg_holdout.amp_max,\n",
    "        \"best_epoch\": int(out_holdout[\"best_epoch\"]),\n",
    "        \"best_val_ce\": float(out_holdout[\"best_val_ce\"]),\n",
    "        \"val_ce_teacher_forced\": float(val_ce),\n",
    "        \"test_ce_teacher_forced\": float(test_ce),\n",
    "        \"val_metrics_expected\": val_m,\n",
    "        \"test_metrics_expected\": test_m,\n",
    "        \"history\": out_holdout[\"history\"],\n",
    "    }\n",
    "    (run_dir / \"holdout_summary.json\").write_text(json.dumps(holdout_summary, indent=2))\n",
    "\n",
    "    # plot holdout CE curves\n",
    "    hh = holdout_summary[\"history\"]\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.plot(hh[\"epoch\"], hh[\"train_ce\"], marker=\"o\", label=\"train_ce\")\n",
    "    plt.plot(hh[\"epoch\"], hh[\"val_ce\"], marker=\"o\", label=\"val_ce\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"cross-entropy\")\n",
    "    plt.title(f\"{months}m HOLDOUT TRAINING (time-safe split)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plots_dir / \"holdout_ce_curves.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    logger.info(f\"[HOLDOUT] val_ce={val_ce:.4f} test_ce={test_ce:.4f} | val_MAE={val_m['MAE']:.4f} test_MAE={test_m['MAE']:.4f}\")\n",
    "\n",
    "    return {\"overfit\": overfit_summary, \"holdout\": holdout_summary}\n",
    "\n",
    "\n",
    "def main():\n",
    "    RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    all_reports = []\n",
    "    for months in WINDOW_MONTHS_LIST:\n",
    "        rep = run_one_months(\n",
    "            months=months,\n",
    "            train_frac=0.70,\n",
    "            val_frac=0.15,\n",
    "            auto_amp_pct=99.9,\n",
    "            overfit_n_files=2,\n",
    "            overfit_fixed_segments=1500,\n",
    "            overfit_epochs=250,\n",
    "            holdout_epochs=200,\n",
    "        )\n",
    "        all_reports.append(rep)\n",
    "\n",
    "    # one-line summary\n",
    "    parts = []\n",
    "    for rep in all_reports:\n",
    "        months = rep[\"holdout\"][\"months\"]\n",
    "        val_ce = rep[\"holdout\"][\"val_ce_teacher_forced\"]\n",
    "        test_ce = rep[\"holdout\"][\"test_ce_teacher_forced\"]\n",
    "        gap = rep[\"overfit\"][\"gap_fixed_train_minus_real_val\"]\n",
    "        parts.append(f\"{months}m(valCE={val_ce:.3f}, testCE={test_ce:.3f}, overfitGap={gap:.3f})\")\n",
    "    print(\" | \".join(parts))\n",
    "    print(f\"[DONE] Outputs saved under: {RUNS_DIR.resolve()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
